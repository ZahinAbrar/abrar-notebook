
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Sparse Attention ‚Äî From Full Attention to BigBird | Abrar Zahin</title>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]});">
</script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"
  onload="hljs.highlightAll();"></script>

<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Lora:ital,wght@0,400;0,600;1,400&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

<style>
  *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
  :root {
    --text: #1a1a1a; --muted: #6b7280; --border: #e5e7eb;
    --bg: #ffffff; --code-bg: #f6f8fa; --link: #0070f3;
    --accent: #0070f3; --toc-active: #0070f3; --hover-bg: #f9fafb;
  }
  html { font-size: 16px; }
  body { background: var(--bg); color: var(--text); font-family: 'Inter', -apple-system, sans-serif; line-height: 1.75; -webkit-font-smoothing: antialiased; }
  a { color: var(--link); text-decoration: none; }
  a:hover { text-decoration: underline; }

  header { border-bottom: 1px solid var(--border); padding: 0 1.5rem; height: 52px; display: flex; align-items: center; justify-content: space-between; position: sticky; top: 0; background: rgba(255,255,255,0.92); backdrop-filter: blur(8px); z-index: 100; }
  .logo { font-weight: 700; font-size: 1.05rem; color: var(--text); }
  nav { display: flex; gap: 1.25rem; align-items: center; }
  nav a { font-size: 0.875rem; color: var(--text); }
  nav a:hover { text-decoration: none; color: var(--accent); }

  .breadcrumb { max-width: 1080px; margin: 0 auto; padding: 0.9rem 1.5rem 0; font-size: 0.78rem; color: var(--muted); display: flex; gap: 0.4rem; align-items: center; }
  .breadcrumb a { color: var(--muted); }
  .breadcrumb a:hover { color: var(--text); text-decoration: none; }
  .breadcrumb-sep { color: var(--border); }

  .page-layout { max-width: 1080px; margin: 0 auto; padding: 2rem 1.5rem 3rem; display: grid; grid-template-columns: 1fr 240px; gap: 4rem; align-items: start; }
  main { min-width: 0; }

  .article-meta { font-size: 0.82rem; color: var(--muted); margin-bottom: 1.6rem; }
  .article-meta span + span::before { content: ' | '; }

  .article-title { font-family: 'Lora', Georgia, serif; font-size: 1.95rem; font-weight: 600; line-height: 1.25; letter-spacing: -0.02em; margin-bottom: 0.35rem; }
  .article-subtitle { font-size: 0.92rem; color: var(--muted); margin-bottom: 1.8rem; font-style: italic; }

  .abstract-box { border: 1px solid var(--border); border-radius: 6px; padding: 1.1rem 1.3rem; margin: 1.3rem 0 1.8rem; background: #fafafa; }
  .abstract-label { font-size: 0.68rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.1em; color: var(--muted); margin-bottom: 0.5rem; }
  .abstract-box p { font-size: 0.9rem; line-height: 1.75; color: #374151; }

  .note-box { background: #eff6ff; border-left: 3px solid #3b82f6; padding: 0.75rem 1rem; font-size: 0.85rem; color: #1e40af; margin: 1.2rem 0; border-radius: 0 4px 4px 0; line-height: 1.6; }
  .warning-box { background: #fefce8; border-left: 3px solid #fbbf24; padding: 0.75rem 1rem; font-size: 0.85rem; color: #92400e; margin: 1.2rem 0; border-radius: 0 4px 4px 0; line-height: 1.6; }

  .article-body h1 { font-family: 'Lora', serif; font-size: 1.42rem; font-weight: 600; margin: 2.2rem 0 0.85rem; scroll-margin-top: 78px; padding-bottom: 0.35rem; border-bottom: 1px solid var(--border); }
  .article-body h2 { font-family: 'Lora', serif; font-size: 1.12rem; font-weight: 600; margin: 1.7rem 0 0.55rem; scroll-margin-top: 78px; }
  .article-body h3 { font-family: 'Inter', sans-serif; font-size: 0.98rem; font-weight: 600; margin: 1.2rem 0 0.45rem; scroll-margin-top: 78px; }
  .article-body p { font-size: 0.95rem; margin-bottom: 0.95rem; color: #1f2937; line-height: 1.8; }
  .article-body strong { font-weight: 600; }
  .article-body em { font-style: italic; }

  .article-body code { font-family: 'JetBrains Mono', monospace; font-size: 0.83em; background: var(--code-bg); padding: 0.12em 0.38em; border-radius: 3px; border: 1px solid #e2e8f0; color: #c7254e; }
  .article-body pre { background: var(--code-bg) !important; border: 1px solid var(--border); border-radius: 6px; padding: 0 !important; overflow: hidden; margin: 1.2rem 0; }
  .code-header { display: flex; justify-content: space-between; align-items: center; background: #f0f2f5; border-bottom: 1px solid var(--border); padding: 0.45rem 1rem; font-size: 0.72rem; font-family: 'JetBrains Mono', monospace; color: var(--muted); }
  .code-lang { background: #e5e7eb; padding: 1px 7px; border-radius: 3px; font-size: 0.68rem; text-transform: uppercase; letter-spacing: 0.05em; }
  .article-body pre code { background: none !important; border: none !important; padding: 1rem 1.2rem !important; font-size: 0.82rem !important; color: #111827 !important; display: block; overflow-x: auto; line-height: 1.65; }

  .article-body ul, .article-body ol { padding-left: 1.6rem; margin-bottom: 1rem; }
  .article-body li { font-size: 0.95rem; margin-bottom: 0.35rem; line-height: 1.7; }

  .article-body table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; }
  .article-body table th { padding: 0.75rem; text-align: left; border: 1px solid var(--border); background: #f3f4f6; font-weight: 600; font-size: 0.875rem; }
  .article-body table td { padding: 0.75rem; border: 1px solid var(--border); font-size: 0.875rem; }
  .article-body table tr:nth-child(even) { background: #f9fafb; }

  .part-divider { border-top: 3px solid var(--border); margin: 3rem 0; padding-top: 0.5rem; }
  .part-title { font-family: 'Lora', serif; font-size: 1.6rem; font-weight: 600; color: var(--accent); margin-bottom: 0.5rem; }

  /* Diagram styles */
  .diagram-wrap { margin: 1.5rem 0; border: 1px solid var(--border); border-radius: 6px; overflow: hidden; background: #fafafa; padding: 1.4rem 1rem 1rem; text-align: center; }
  .diagram-caption { font-size: 0.78rem; color: var(--muted); margin-top: 0.75rem; font-style: italic; text-align: center; }

  .legend { display: flex; gap: 1.1rem; flex-wrap: wrap; justify-content: center; margin-top: 0.85rem; font-size: 0.78rem; color: #374151; }
  .legend-item { display: flex; align-items: center; gap: 0.35rem; }
  .legend-dot { width: 13px; height: 13px; border-radius: 2px; flex-shrink: 0; }

  /* References */
  .references { border-top: 1px solid var(--border); margin-top: 2.5rem; padding-top: 1.5rem; }
  .ref-label { font-size: 0.68rem; font-weight: 600; text-transform: uppercase; letter-spacing: 0.1em; color: var(--muted); margin-bottom: 0.75rem; }
  .references ol { padding-left: 1.4rem; }
  .references li { font-size: 0.84rem; color: #374151; margin-bottom: 0.5rem; line-height: 1.65; }

  .sidebar { position: sticky; top: 72px; }
  .toc-title { font-weight: 600; font-size: 0.68rem; text-transform: uppercase; letter-spacing: 0.09em; color: var(--muted); margin-bottom: 0.8rem; }
  .toc { list-style: none; padding: 0; border-left: 1px solid var(--border); padding-left: 0.85rem; }
  .toc li { margin-bottom: 0.45rem; line-height: 1.35; }
  .toc a { color: var(--muted); font-size: 0.78rem; transition: color 0.15s; display: block; text-decoration: none; cursor: pointer; }
  .toc a:hover, .toc a.active { color: var(--toc-active); }
  .toc .toc-h2 { padding-left: 0.75rem; }
  .toc-part { font-weight: 600; color: var(--accent); margin-top: 1rem; margin-bottom: 0.5rem; font-size: 0.72rem; }

  .back-link { display: inline-flex; align-items: center; gap: 0.4rem; font-size: 0.85rem; color: var(--muted); cursor: pointer; margin-top: 2.2rem; padding-top: 1.5rem; border-top: 1px solid var(--border); width: 100%; }
  .back-link:hover { color: var(--text); text-decoration: none; }

  footer { border-top: 1px solid var(--border); padding: 1.5rem; text-align: center; font-size: 0.78rem; color: var(--muted); margin-top: 2.5rem; }

  @media (max-width: 900px) {
    .page-layout { grid-template-columns: 1fr; }
    .sidebar { display: none; }
  }
</style>
</head>

<body>
<header>
  <a class="logo" href="../index.html">Abrar Zahin</a>
  <nav>
    <a href="../index.html">Notebook</a>
    <a href="https://zahinabrar.github.io/">About</a>
    <a href="https://zahinabrar.github.io/projects/">Projects</a>
    <a href="https://zahinabrar.github.io/publications/">Publications</a>
  </nav>
</header>

<div class="breadcrumb">
  <a href="../index.html">Home</a>
  <span class="breadcrumb-sep">‚Ä∫</span>
  <a href="../index.html">Notebook</a>
  <span class="breadcrumb-sep">‚Ä∫</span>
  <span style="color:var(--text);">Sparse Attention ‚Äî BigBird</span>
</div>

<div class="page-layout">
  <main>
    <article>
      <div class="article-meta">
        <span>Feb 2026</span>
        <span>Deep Dive</span>
        <span>Abrar Zahin</span>
        <span>Transformers ¬∑ Attention</span>
      </div>

      <h1 class="article-title">Sparse Attention ‚Äî From Full Attention to BigBird</h1>
      <div class="article-subtitle">Why standard attention collapses on long sequences, and how BigBird's block-sparse mechanism ‚Äî combining global, sliding, and random patterns ‚Äî tames it in linear time.</div>

      <div class="abstract-box">
        <div class="abstract-label">Abstract</div>
        <p>Standard transformer attention computes an $n \times n$ score matrix ‚Äî $O(n^2)$ time and memory. This makes sequences beyond ~512 tokens prohibitively expensive. This post traces the problem from first principles, builds intuition for three complementary sparse attention patterns, and explains how BigBird (Zaheer et al., 2020) combines them into a single block-sparse mechanism at $O(n)$ complexity. Conceptually based on the <a href="https://arxiv.org/abs/2007.14062" target="_blank">original BigBird paper</a> and the <a href="https://huggingface.co/blog/big-bird" target="_blank">Hugging Face blog post</a> by Vasudev Gupta.</p>
      </div>

      <div class="article-body">

        <!-- ===================== PART 1 ===================== -->
        <div class="part-divider">
          <div class="part-title">Part 1: The Quadratic Wall</div>
        </div>

        <h1 id="full-attention">Standard (Full) Attention</h1>
        <p>In a vanilla transformer, every token attends to every other token. Given a sequence of $n$ tokens represented by queries $Q$, keys $K$, and values $V$, the standard formulation is:</p>

        <p>$$\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$</p>

        <p>The bottleneck is the $QK^\top$ term ‚Äî an $n \times n$ matrix. Every row $i$ requires computing a dot product between query $q_i$ and all $n$ keys. So scaling sequence length from 512 to 1024 doesn't double the cost ‚Äî it quadruples it.</p>

        <h2 id="cost-explosion">The Cost Explosion</h2>

        <table>
          <tr><th>Sequence Length</th><th>Relative Compute vs. 512</th><th>Fits in GPU memory?</th></tr>
          <tr><td>512 (BERT default)</td><td>1√ó</td><td>‚úÖ Comfortably</td></tr>
          <tr><td>1,024</td><td>4√ó</td><td>‚ö†Ô∏è Tight</td></tr>
          <tr><td>2,048</td><td>16√ó</td><td>‚ùå Usually not</td></tr>
          <tr><td>4,096</td><td>64√ó</td><td>‚ùå Out of memory</td></tr>
        </table>

        <div class="warning-box">
          Simply truncating long documents to 512 tokens throws away most of the content. For tasks like legal document review or scientific QA, the key evidence is often hundreds of tokens away from where you'd first look.
        </div>

        <h1 id="question">The Key Question</h1>
        <p>Does every token really <em>need</em> to attend to every other token? Or is the full $n \times n$ matrix mostly near-zero anyway?</p>

        <p>Think about how language works. When you read the word "running", the most relevant context is the subject before it and the object after it ‚Äî not every word in a 5,000-token document equally. Most of the $n^2$ attention weights are tiny. We're computing and storing them anyway, paying the full $O(n^2)$ cost for information that adds almost nothing.</p>

        <p>This motivates <strong>sparse attention</strong>: carefully choose a structured subset of token pairs to attend, skip the rest, and preserve almost all of the representational power at a fraction of the cost.</p>


        <!-- ===================== PART 2 ===================== -->
        <div class="part-divider">
          <div class="part-title">Part 2: Three Complementary Sparse Patterns</div>
        </div>

        <h1 id="three-patterns">Building Up the Intuition</h1>
        <p>BigBird's insight is that three distinct attention patterns, each cheap individually, together approximate full attention well. Let's understand each one from scratch.</p>

        <h2 id="sliding">1. Sliding (Local) Attention</h2>
        <p>The most natural sparsity: token $i$ attends only to tokens within a fixed window $[i - w/2,\; i + w/2]$ around it. This directly captures local syntactic structure ‚Äî subject-verb agreement, adjacent adjectives, nearby prepositions ‚Äî the kinds of relationships that dominate grammar.</p>

        <!-- Sliding attention SVG -->
        <div class="diagram-wrap">
          <svg viewBox="0 0 500 150" width="100%" xmlns="http://www.w3.org/2000/svg" style="max-width:500px;display:block;margin:0 auto;">
            <defs>
              <marker id="arr" markerWidth="6" markerHeight="6" refX="3" refY="3" orient="auto">
                <path d="M0,0 L6,3 L0,6 Z" fill="#f59e0b"/>
              </marker>
            </defs>
            <!-- Labels top row -->
            <text x="40"  y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T1</text>
            <text x="100" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T2</text>
            <text x="160" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T3</text>
            <text x="220" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T4</text>
            <text x="280" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T5</text>
            <text x="340" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T6</text>
            <text x="400" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T7</text>
            <text x="460" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T8</text>
            <!-- Query nodes -->
            <circle cx="40"  cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="100" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="160" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="220" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="280" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="340" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="400" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="460" cy="45" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <!-- Sliding connections window=3 -->
            <!-- T3 (center example, highlight) ‚Üí T2, T3, T4 -->
            <line x1="160" y1="58" x2="100" y2="92" stroke="#f59e0b" stroke-width="2"/>
            <line x1="160" y1="58" x2="160" y2="92" stroke="#f59e0b" stroke-width="2"/>
            <line x1="160" y1="58" x2="220" y2="92" stroke="#f59e0b" stroke-width="2"/>
            <!-- T4 ‚Üí T3,T4,T5 -->
            <line x1="220" y1="58" x2="160" y2="92" stroke="#f59e0b" stroke-width="1.5"/>
            <line x1="220" y1="58" x2="220" y2="92" stroke="#f59e0b" stroke-width="1.5"/>
            <line x1="220" y1="58" x2="280" y2="92" stroke="#f59e0b" stroke-width="1.5"/>
            <!-- T1 ‚Üí T1,T2 -->
            <line x1="40"  y1="58" x2="40"  y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="40"  y1="58" x2="100" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- T2 ‚Üí T1,T2,T3 -->
            <line x1="100" y1="58" x2="40"  y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="100" y1="58" x2="100" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="100" y1="58" x2="160" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- T5 ‚Üí T4,T5,T6 -->
            <line x1="280" y1="58" x2="220" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="280" y1="58" x2="280" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="280" y1="58" x2="340" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- T6 ‚Üí T5,T6,T7 -->
            <line x1="340" y1="58" x2="280" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="340" y1="58" x2="340" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="340" y1="58" x2="400" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- T7 ‚Üí T6,T7,T8 -->
            <line x1="400" y1="58" x2="340" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="400" y1="58" x2="400" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="400" y1="58" x2="460" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- T8 ‚Üí T7,T8 -->
            <line x1="460" y1="58" x2="400" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <line x1="460" y1="58" x2="460" y2="92" stroke="#f59e0b" stroke-width="1.2" opacity="0.5"/>
            <!-- Key nodes -->
            <circle cx="40"  cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="100" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="160" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="220" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="280" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="340" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="400" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <circle cx="460" cy="105" r="13" fill="#fde68a" stroke="#f59e0b" stroke-width="1.5"/>
            <text x="250" y="138" font-size="9" fill="#6b7280" text-anchor="middle" font-family="Inter,sans-serif">Sliding attention (window = 3): attend only to immediate neighbors</text>
          </svg>
        </div>

        <p>Cost: $O(w \cdot n)$ where $w$ is the window size ‚Äî a constant. So total cost is just $O(n)$. But there's a catch: a token at position 10 has no direct path to a token at position 500. Long-range dependencies are unreachable in a single layer.</p>

        <h2 id="global">2. Global Attention</h2>
        <p>Designate a small set of <strong>global tokens</strong>. Each global token attends to every token in the sequence, and every token attends back to each global token. They act as high-bandwidth relay nodes ‚Äî any two tokens can exchange information in at most two hops through a global token.</p>

        <!-- Global attention SVG -->
        <div class="diagram-wrap">
          <svg viewBox="0 0 500 150" width="100%" xmlns="http://www.w3.org/2000/svg" style="max-width:500px;display:block;margin:0 auto;">
            <!-- Labels -->
            <text x="40"  y="16" font-size="9" fill="#0070f3" font-family="JetBrains Mono,monospace" text-anchor="middle" font-weight="700">G1</text>
            <text x="100" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T2</text>
            <text x="160" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T3</text>
            <text x="220" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T4</text>
            <text x="280" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T5</text>
            <text x="340" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T6</text>
            <text x="400" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T7</text>
            <text x="460" y="16" font-size="9" fill="#0070f3" font-family="JetBrains Mono,monospace" text-anchor="middle" font-weight="700">G2</text>
            <!-- Global query nodes (filled blue) -->
            <circle cx="40"  cy="45" r="13" fill="#0070f3"/>
            <circle cx="100" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="160" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="220" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="280" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="340" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="400" cy="45" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="460" cy="45" r="13" fill="#0070f3"/>
            <!-- G1 attends all keys -->
            <line x1="40" y1="58" x2="40"  y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="100" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="160" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="220" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="280" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="340" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="400" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="40" y1="58" x2="460" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <!-- G2 attends all keys -->
            <line x1="460" y1="58" x2="40"  y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="100" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="160" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="220" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="280" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="340" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="400" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <line x1="460" y1="58" x2="460" y2="92" stroke="#0070f3" stroke-width="1.8" stroke-dasharray="4,2"/>
            <!-- Non-global tokens attend both global keys -->
            <line x1="100" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="100" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="160" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="160" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="220" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="220" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="280" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="280" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="340" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="340" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="400" y1="58" x2="40"  y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <line x1="400" y1="58" x2="460" y2="92" stroke="#93c5fd" stroke-width="1.2"/>
            <!-- Key nodes -->
            <circle cx="40"  cy="105" r="13" fill="#0070f3"/>
            <circle cx="100" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="160" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="220" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="280" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="340" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="400" cy="105" r="13" fill="#dbeafe" stroke="#93c5fd" stroke-width="1.5"/>
            <circle cx="460" cy="105" r="13" fill="#0070f3"/>
            <text x="250" y="138" font-size="9" fill="#6b7280" text-anchor="middle" font-family="Inter,sans-serif">Global tokens (blue) attend all; all tokens attend global tokens ‚Äî bidirectional hubs</text>
          </svg>
        </div>

        <p>Cost: $O(g \cdot n)$ where $g$ is the number of global tokens, typically very small (2‚Äì4 blocks). Anything that needs to "talk across" the whole sequence can do so in two hops: first ‚Üí global ‚Üí second.</p>

        <p>The choice of global tokens depends on the task. For question-answering, making the entire question global is natural ‚Äî every context token should directly compare itself to the question without having to relay through sliding neighbors.</p>

        <h2 id="random">3. Random Attention</h2>
        <p>Each token also attends to a handful of <strong>randomly chosen</strong> tokens from elsewhere in the sequence. This has a graph-theoretic justification: in a purely local (sliding) graph, the diameter ‚Äî the worst-case path between any two nodes ‚Äî is $O(n/w)$. Randomly adding $r$ long-range edges per node drops the average path length to $O(\log n)$, the classic small-world network effect.</p>

        <!-- Random attention SVG -->
        <div class="diagram-wrap">
          <svg viewBox="0 0 500 150" width="100%" xmlns="http://www.w3.org/2000/svg" style="max-width:500px;display:block;margin:0 auto;">
            <text x="40"  y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T1</text>
            <text x="100" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T2</text>
            <text x="160" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T3</text>
            <text x="220" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T4</text>
            <text x="280" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T5</text>
            <text x="340" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T6</text>
            <text x="400" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T7</text>
            <text x="460" y="16" font-size="9" fill="#6b7280" font-family="JetBrains Mono,monospace" text-anchor="middle">T8</text>
            <circle cx="40"  cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="100" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="160" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="220" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="280" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="340" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="400" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="460" cy="45" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <!-- Random long-range connections -->
            <line x1="100" y1="58" x2="340" y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <line x1="160" y1="58" x2="460" y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <line x1="220" y1="58" x2="40"  y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <line x1="280" y1="58" x2="100" y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <line x1="340" y1="58" x2="160" y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <line x1="400" y1="58" x2="220" y2="92" stroke="#10b981" stroke-width="2" stroke-dasharray="5,3"/>
            <circle cx="40"  cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="100" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="160" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="220" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="280" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="340" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="400" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <circle cx="460" cy="105" r="13" fill="#d1fae5" stroke="#10b981" stroke-width="1.5"/>
            <text x="250" y="138" font-size="9" fill="#6b7280" text-anchor="middle" font-family="Inter,sans-serif">Random attention: each token picks a few random distant keys ‚Äî creates information shortcuts</text>
          </svg>
        </div>

        <p>Cost: $O(r \cdot n)$ where $r$ is typically 3 random keys per token. Individually, random attention is the least powerful of the three ‚Äî but combined with sliding and global, it fills in the gaps and prevents information bottlenecks.</p>


        <!-- ===================== PART 3 ===================== -->
        <div class="part-divider">
          <div class="part-title">Part 3: BigBird ‚Äî Combining All Three</div>
        </div>

        <h1 id="bigbird">BigBird Block Sparse Attention</h1>
        <p>BigBird's core idea is elegantly simple: for each query token, gather the keys from all three attention types ‚Äî global, sliding, and random ‚Äî and compute a single attention score over that union. The result is an approximation of full attention with theoretical guarantees.</p>

        <pre><div class="code-header"><span>what each query token attends to</span><span class="code-lang">text</span></div><code>For token i, the attended key set is:

  key_set(i) = global_tokens          # long-range routing
              ‚à™ {i-1, i, i+1}         # sliding window (w=3)
              ‚à™ {r1, r2, r3}          # random keys

Total: g + w + r keys  ‚Üí  O(1) per token  ‚Üí  O(n) total</code></pre>

        <h2 id="block">The "Block" Part</h2>
        <p>BigBird doesn't operate token-by-token but in <strong>blocks of $b$ tokens</strong> (typically $b = 64$). All three patterns are defined at block granularity ‚Äî a global block means an entire chunk of 64 tokens that attends/is-attended-by all other blocks. Sliding means neighboring blocks. Random means random block selections.</p>

        <p>Why blocks? Modern GPUs excel at dense matrix multiplications. A token-level sparse operation would produce many tiny irregularly-shaped computations that hardware can't batch efficiently. Block-level sparsity lets BigBird represent each attention pattern as a series of small dense $b \times b$ matrix multiplications ‚Äî a form hardware loves.</p>

        <h2 id="attn-matrix">The Attention Matrix Visualized</h2>

        <div class="diagram-wrap">
          <div style="display:flex;gap:2.5rem;justify-content:center;flex-wrap:wrap;align-items:flex-start;">
            <!-- Full attention -->
            <div>
              <div style="font-size:0.8rem;font-weight:600;color:#374151;text-align:center;margin-bottom:8px;">Full Attention ‚Äî $O(n^2)$</div>
              <div style="display:grid;grid-template-columns:repeat(8,28px);gap:3px;">
                <!-- 8x8 all blue -->
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
                <div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div><div style="width:28px;height:28px;border-radius:3px;background:#3b82f6;"></div>
              </div>
              <div style="font-size:0.72rem;color:var(--muted);text-align:center;margin-top:6px;">64 cells computed</div>
            </div>
            <!-- Sparse BigBird -->
            <div>
              <div style="font-size:0.8rem;font-weight:600;color:#374151;text-align:center;margin-bottom:8px;">BigBird Sparse ‚Äî $O(n)$</div>
              <div style="display:grid;grid-template-columns:repeat(8,28px);gap:3px;">
                <!-- Row 0: global ‚Üí all -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;" title="global"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 1: global, sliding 0-2, random 5, global -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 2 -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 3 -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 4 -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 5 -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 6 -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#10b981;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#e5e7eb;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#f59e0b;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
                <!-- Row 7: global ‚Üí all -->
                <div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div><div style="width:28px;height:28px;border-radius:3px;background:#0070f3;"></div>
              </div>
              <div style="font-size:0.72rem;color:var(--muted);text-align:center;margin-top:6px;">~30 cells computed ‚Äî 53% reduction</div>
            </div>
          </div>
          <div class="legend">
            <div class="legend-item"><div class="legend-dot" style="background:#0070f3;"></div><span>Global</span></div>
            <div class="legend-item"><div class="legend-dot" style="background:#f59e0b;"></div><span>Sliding</span></div>
            <div class="legend-item"><div class="legend-dot" style="background:#10b981;"></div><span>Random</span></div>
            <div class="legend-item"><div class="legend-dot" style="background:#e5e7eb;"></div><span>Skipped</span></div>
          </div>
          <div class="diagram-caption">At $n=8$ the savings look modest; at $n=4096$ the right matrix represents 8√ó fewer computations than the left.</div>
        </div>

        <h1 id="edge-blocks">Special Handling at Sequence Boundaries</h1>
        <p>Not all positions are treated the same. The first and last blocks are made fully global ‚Äî they attend to everything and everything attends to them. This is important because sequence boundaries often contain special tokens (like <code>[CLS]</code> or task prompts) that the whole sequence should be aware of.</p>

        <p>The second and second-to-last blocks also get special treatment. Their position is close enough to the boundary that they need richer context than a middle block, but not so much that making them fully global would be justified. BigBird computes their scores by explicitly gathering the first few blocks, the last block, the sliding neighbors, and one random block.</p>

        <h1 id="complexity">Complexity in Numbers</h1>

        <table>
          <tr><th>Attention</th><th>Complexity</th><th>Cost at $n=4096$ vs. BERT-512</th></tr>
          <tr><td>BERT (full)</td><td>$O(n^2)$</td><td>64√ó</td></tr>
          <tr><td>BigBird (block sparse)</td><td>$O(n)$</td><td>~8√ó</td></tr>
          <tr><td>Longformer (sliding+global)</td><td>$O(n)$</td><td>~8√ó</td></tr>
        </table>

        <pre><div class="code-header"><span>concrete numbers (b=64, g=2, s=3, r=3)</span><span class="code-lang">text</span></div><code>Keys attended per query block = (g + s + r) √ó b
                              = (2 + 3 + 3) √ó 64 = 512 keys

For full attention at n=4096: 4096 keys per query
For BigBird at n=4096:          512 keys per query
                              ‚Üí  8√ó fewer per block, O(n) total

Compared to BERT at n=512 (our baseline T):
  BERT at 4096:    (4096/512)¬≤ √ó T = 64T
  BigBird at 4096: (4096/512)  √ó T =  8T  ‚Üê same ballpark as BERT!</code></pre>


        <!-- ===================== PART 4 ===================== -->
        <div class="part-divider">
          <div class="part-title">Part 4: ITC vs ETC and Practical Usage</div>
        </div>

        <h1 id="itc-etc">Two Training Strategies</h1>
        <p>BigBird comes in two flavors based on how global tokens are defined:</p>

        <h2 id="itc">ITC ‚Äî Internal Transformer Construction</h2>
        <p>Global tokens are chosen from within the actual input sequence ‚Äî typically the first and last blocks. No extra tokens are prepended. The model learns to use these internal positions as global information hubs. ITC is the cheaper variant and works well for general long-document tasks like classification and summarization.</p>

        <h2 id="etc">ETC ‚Äî Extended Transformer Construction</h2>
        <p>Additional special tokens are prepended to the input sequence and designated as global. For question-answering, the full question can be encoded as global tokens ‚Äî meaning every single context token directly attends to and is attended by the question. This gives the model richer task-specific global context, at the cost of slightly more computation.</p>

        <div class="note-box">
          In ETC experiments, random blocks are often set to zero. With many global tokens already providing short-path routing, the small-world shortcut benefit of random attention becomes redundant.
        </div>

        <table>
          <tr><th></th><th>ITC</th><th>ETC</th></tr>
          <tr><td><strong>Global tokens</strong></td><td>Internal (first/last blocks)</td><td>Prepended special tokens</td></tr>
          <tr><td><strong>Random blocks</strong></td><td>Yes ($r = 3$ typical)</td><td>Often 0</td></tr>
          <tr><td><strong>Compute cost</strong></td><td>Lower</td><td>Slightly higher</td></tr>
          <tr><td><strong>Best suited for</strong></td><td>Summarization, classification</td><td>Long-context QA</td></tr>
        </table>

        <h1 id="hf">Using BigBird with ü§ó Transformers</h1>
        <p>BigBird is available in the Hugging Face Transformers library and mirrors BERT's API closely. The only meaningful change in your code is allowing much longer inputs.</p>

        <pre><div class="code-header"><span>question answering with BigBird</span><span class="code-lang">python</span></div><code>from transformers import BigBirdTokenizer, BigBirdForQuestionAnswering
import torch

model_name = "google/bigbird-roberta-base"
tokenizer  = BigBirdTokenizer.from_pretrained(model_name)
model      = BigBirdForQuestionAnswering.from_pretrained(model_name)

question = "What problem does block sparse attention solve?"
context  = "..." * 300  # ~3000 token document ‚Äî no problem for BigBird

inputs = tokenizer(
    question,
    context,
    return_tensors="pt",
    max_length=4096,      # BigBird handles 4096 tokens natively
    truncation=True,
    padding="max_length"
)

with torch.no_grad():
    outputs = model(**inputs)

start  = outputs.start_logits.argmax()
end    = outputs.end_logits.argmax()
answer = tokenizer.decode(inputs["input_ids"][0][start : end + 1])
print(answer)</code></pre>

        <div class="note-box">
          The Transformers library automatically switches BigBird between <code>block_sparse</code> and <code>original_full</code> attention based on sequence length. Below ~1024 tokens, full attention is used since the overhead of the block-sparse implementation isn't justified at that scale.
        </div>

        <h1 id="when">When to Use BigBird vs. BERT</h1>
        <p>BigBird is an <em>approximation</em> of full attention. For short sequences, BERT's exact attention will almost always win on quality. Use BigBird when:</p>
        <ul>
          <li>Inputs are <strong>consistently above 512‚Äì1024 tokens</strong></li>
          <li>The answer or label depends on context <strong>spread far apart</strong> in the document</li>
          <li>Truncation would discard meaningful content</li>
          <li>Tasks: long-context QA, document summarization, scientific paper understanding, genomics</li>
        </ul>


        <!-- ===================== PART 5 ===================== -->
        <div class="part-divider">
          <div class="part-title">Part 5: Summary</div>
        </div>

        <h1 id="summary">The Full Picture</h1>

        <pre><div class="code-header"><span>BigBird in one mental model</span><span class="code-lang">text</span></div><code>PROBLEM
  Full attention = O(n¬≤) ‚Üí explodes for long sequences

OBSERVATION
  Most attention weights are near-zero anyway ‚Üí sparsity is fine

THREE SPARSE PATTERNS, each O(n):
  ‚ë† Sliding   ‚Üí captures local grammar, neighboring context
  ‚ë° Global    ‚Üí long-range routing, any two tokens ‚â§ 2 hops apart
  ‚ë¢ Random    ‚Üí small-world shortcuts, reduces worst-case path length

BIGBIRD BLOCK SPARSE
  Combines all three at block granularity (b=64 tokens per block)
  ‚Üí Dense b√ób matrix multiplications = hardware-efficient
  ‚Üí O((g + s + r) √ó n) = O(n) total

RESULT
  4096 tokens at roughly the same cost as BERT at ~2000
  Theoretical guarantee: sparse attention is as expressive as full
  SOTA on long-doc QA, summarization, genomics tasks</code></pre>

        <h1 id="key-params">Key Parameters Reference</h1>

        <table>
          <tr><th>Parameter</th><th>Symbol</th><th>Typical value</th><th>What it controls</th></tr>
          <tr><td>Block size</td><td>$b$</td><td>64</td><td>Tokens per attention block</td></tr>
          <tr><td>Global blocks</td><td>$g$</td><td>2</td><td>Blocks with full global attention</td></tr>
          <tr><td>Sliding blocks</td><td>$s$</td><td>3</td><td>Local window (3 = self + 1 neighbor each side)</td></tr>
          <tr><td>Random blocks</td><td>$r$</td><td>3</td><td>Random key blocks per query block</td></tr>
          <tr><td>Max seq length</td><td>$n$</td><td>4096</td><td>8√ó BERT's default 512</td></tr>
        </table>

        <div class="references">
          <div class="ref-label">References</div>
          <ol>
            <li>Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). <em>Big Bird: Transformers for Longer Sequences.</em> NeurIPS 2020. <a href="https://arxiv.org/abs/2007.14062" target="_blank">arxiv.org/abs/2007.14062</a></li>
            <li>Gupta, V. (2021). <em>Understanding BigBird's Block Sparse Attention.</em> Hugging Face Blog. <a href="https://huggingface.co/blog/big-bird" target="_blank">huggingface.co/blog/big-bird</a></li>
            <li>Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). <em>Efficient Transformers: A Survey.</em> <a href="https://arxiv.org/abs/2009.06732" target="_blank">arxiv.org/abs/2009.06732</a></li>
            <li>Beltagy, I., Peters, M. E., & Cohan, A. (2020). <em>Longformer: The Long-Document Transformer.</em> <a href="https://arxiv.org/abs/2004.05150" target="_blank">arxiv.org/abs/2004.05150</a></li>
          </ol>
        </div>

        <a class="back-link" href="../index.html">‚Üê Back to Notebook</a>
      </div>
    </article>
  </main>

  <aside class="sidebar">
    <div class="toc-title">Table of Contents</div>
    <ul class="toc">
      <li class="toc-part">Part 1: The Problem</li>
      <li><a onclick="tocScroll('full-attention')">Standard Attention</a></li>
      <li class="toc-h2"><a onclick="tocScroll('cost-explosion')">Cost Explosion</a></li>
      <li><a onclick="tocScroll('question')">Do We Need All Tokens?</a></li>

      <li class="toc-part">Part 2: Sparse Patterns</li>
      <li><a onclick="tocScroll('three-patterns')">Three Patterns</a></li>
      <li class="toc-h2"><a onclick="tocScroll('sliding')">Sliding Attention</a></li>
      <li class="toc-h2"><a onclick="tocScroll('global')">Global Attention</a></li>
      <li class="toc-h2"><a onclick="tocScroll('random')">Random Attention</a></li>

      <li class="toc-part">Part 3: BigBird</li>
      <li><a onclick="tocScroll('bigbird')">Block Sparse Attention</a></li>
      <li class="toc-h2"><a onclick="tocScroll('block')">Why Blocks?</a></li>
      <li class="toc-h2"><a onclick="tocScroll('attn-matrix')">Attention Matrix</a></li>
      <li><a onclick="tocScroll('edge-blocks')">Boundary Handling</a></li>
      <li><a onclick="tocScroll('complexity')">Complexity</a></li>

      <li class="toc-part">Part 4: Practice</li>
      <li><a onclick="tocScroll('itc-etc')">ITC vs ETC</a></li>
      <li class="toc-h2"><a onclick="tocScroll('itc')">ITC</a></li>
      <li class="toc-h2"><a onclick="tocScroll('etc')">ETC</a></li>
      <li><a onclick="tocScroll('hf')">HuggingFace Usage</a></li>
      <li><a onclick="tocScroll('when')">When to Use</a></li>

      <li class="toc-part">Part 5: Summary</li>
      <li><a onclick="tocScroll('summary')">Summary</a></li>
      <li><a onclick="tocScroll('key-params')">Key Parameters</a></li>
    </ul>
  </aside>
</div>

<footer>
  ¬© 2026 <a href="https://zahinabrar.github.io/">Abrar Zahin</a>
</footer>

<script>
  function tocScroll(id) {
    document.getElementById(id).scrollIntoView({ behavior: 'smooth' });
  }
  window.addEventListener('scroll', () => {
    const headings = document.querySelectorAll('.article-body h1[id], .article-body h2[id]');
    let active = null;
    headings.forEach(h => { if (h.getBoundingClientRect().top < 95) active = h.id; });
    document.querySelectorAll('.toc a').forEach(a => {
      a.classList.toggle('active', a.getAttribute('onclick') === `tocScroll('${active}')`);
    });
  }, { passive: true });
</script>
</body>
</html>
